{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要下载的文件： \n",
    "\n",
    "- dnabert fine-tuned model\n",
    "\n",
    "    https://drive.google.com/file/d/1BJjqb5Dl2lNMg2warsFQ0-Xvn1xxfFXC/view\n",
    "\n",
    "    ```\n",
    "    unzip 6-new-12w-0.zip\n",
    "    ```\n",
    "\n",
    "# 使用的镜像\n",
    "\n",
    "    pytorch_dnabert\n",
    "\n",
    "# 如果报错transformer就阅读DNABERT的readme，安装相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel, DNATokenizer\n",
    "\n",
    "# pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperParams functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperParameter:\n",
    "    def __init__(self):\n",
    "        self.identity_level = 0.8\n",
    "        self.shuffle_times = 1\n",
    "        self.root_dir = '/lizutan/code/MCANet/dataset'#\"/tangzehua/tangzehua/RecCrossNet/dataset\"\n",
    "        # self.dataset_path = \"{}/pipeline_7/identity_{}_shuffle_{}\".format(self.root_dir, self.identity_level, self.shuffle_times)\n",
    "        self.dataset_path = \"{}/pipeline_10/rec_{}_shuffle_{}\".format(self.root_dir, self.identity_level, self.shuffle_times)\n",
    "        self.dnabert_embed_out_dir = \"/lizutan/code/MCANet/preprocess/dnabert_embedding/pipeline_10/rec_{}_shuffle_{}\".format(self.identity_level, self.shuffle_times)\n",
    "        self.kmer = 6\n",
    "        self.model_path = \"/lizutan/code/MCANet/preprocess/DNAbert_file/6-new-12w-0\"\n",
    "\n",
    "\n",
    "DefaultArgs = hyperParameter()\n",
    "\n",
    "if os.path.exists(DefaultArgs.dnabert_embed_out_dir):\n",
    "    os.rmdir(DefaultArgs.dnabert_embed_out_dir)\n",
    "    # print('OutputDir is exitsted')\n",
    "else:\n",
    "    os.makedirs(DefaultArgs.dnabert_embed_out_dir)\n",
    "    print('success create dir test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmer_sentence(original_string, kmer=1, stride=1):\n",
    "    # 'eqweqweqweqweqeqeqweqweqe'\n",
    "    # 'e q w e q w e q w e q w e q e q e q w e q w e q e'\n",
    "    if kmer == -1:\n",
    "        return original_string\n",
    "    sentence = \"\"\n",
    "    original_string = original_string.replace(\"\\n\", \"\")\n",
    "    i = 0\n",
    "    while i <= len(original_string)-kmer:\n",
    "        sentence += original_string[i:i+kmer] + \" \"\n",
    "        i += stride\n",
    "    return sentence[:-1].strip(\"\\\"\")\n",
    "#\n",
    "def get_dna_fasta_info(input_hyperparam: hyperParameter):\n",
    "    dataset_table = []\n",
    "    for filename in os.listdir(input_hyperparam.dataset_path):\n",
    "        # input_table = pd.read_csv(join(input_hyperparam.dataset_path, filename), sep=\"\\t\", header=None)\n",
    "        input_table = pd.read_csv(join(input_hyperparam.dataset_path, filename), sep=\"\\t\")\n",
    "        # input_table.columns = ['attP', 'attB', 'Rec', 'attP_str', 'attB_str', 'Rec_str', 'label']\n",
    "        dataset_table.append(input_table)\n",
    "    dataset_table = pd.concat(dataset_table, axis=0)\n",
    "    dataset_table.index = list(range(dataset_table.shape[0]))\n",
    "    dna_fasta_dict = {\"attB\": {}, \"attP\": {}}\n",
    "    for site in dna_fasta_dict.keys():\n",
    "        for index in dataset_table.index:\n",
    "            site_index = dataset_table.loc[index, site]\n",
    "            dna_fasta_dict[site][site_index] = dataset_table.loc[index, \"{}_str\".format(site)]\n",
    "    for site, site_string in dna_fasta_dict.items():\n",
    "        dna_fasta_dict[site] = pd.Series(site_string)\n",
    "    return dna_fasta_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomeDataset:\n",
    "    def __init__(self, data, args: hyperParameter):\n",
    "        self.data = data\n",
    "        self.args_param = args\n",
    "        self.data_index = np.array(data.index)\n",
    "        self.tokenizer = DNATokenizer.from_pretrained('dna'+str(args.kmer), do_lower_case=False)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def pad_sequence(self, sequence):\n",
    "        sequence = sequence[: 50] + max(50 - len(sequence), 0) * \"N\"\n",
    "        return sequence\n",
    "    def __getitem__(self, index):\n",
    "        data_index = self.data_index[index]\n",
    "        raw_sequence = self.pad_sequence(self.data[data_index])\n",
    "        sentence_a = get_kmer_sentence(raw_sequence, self.args_param.kmer)\n",
    "        inputs = self.tokenizer.encode_plus(sentence_a, \n",
    "                                            sentence_b=None, \n",
    "                                            return_tensors='pt', \n",
    "                                            add_special_tokens=True)\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        return data_index, input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根据数据计算DNAbert的嵌入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "6469 / 6469 done!\n",
      "\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "6469 / 6469 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dna_fasta_dict = get_dna_fasta_info(DefaultArgs)\n",
    "\n",
    "for site, site_data in dna_fasta_dict.items():\n",
    "    site_dataset = CustomeDataset(site_data, DefaultArgs)\n",
    "    site_dataloader = DataLoader(site_dataset, \n",
    "                                 shuffle=False, \n",
    "                                 drop_last=False, \n",
    "                                 num_workers=8, \n",
    "                                 batch_size=16)\n",
    "    model = BertModel.from_pretrained(DefaultArgs.model_path, output_hidden_states=True)\n",
    "    model = model.to(\"cuda\")\n",
    "    site_dir = join(DefaultArgs.dnabert_embed_out_dir, site)\n",
    "    if not os.path.exists(site_dir):\n",
    "        os.makedirs(site_dir)\n",
    "    processed = 0\n",
    "    for idx, batch in enumerate(site_dataloader):\n",
    "        index_list, input_array = batch\n",
    "        input_array = input_array.to(\"cuda\")\n",
    "        hidden_states = model(input_array)[-1][-1]\n",
    "        hidden_states = hidden_states.cpu().data.numpy()\n",
    "        for array_id, index_name in enumerate(index_list):\n",
    "            output_file = \"{}/{}.npz\".format(site_dir, index_name)\n",
    "            result = dict(\n",
    "                index = index_name,\n",
    "                representations = hidden_states[array_id]\n",
    "            )\n",
    "            np.savez(output_file, **result)\n",
    "        processed += len(index_list)\n",
    "        print(\"\\r{} / {} done!\".format(processed, len(site_dataset)), end='')\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重新开始=========================>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改可以编码任意DNA序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_dataframe(data):\n",
    "    new_dic = {}\n",
    "    for i in range(len(data)):\n",
    "        new_dic[i] = data[1]\n",
    "    # data_pd = pd.concat([pd.Series(range(len(data))),pd.Series(data)],axis=1)\n",
    "    return pd.Series(new_dic)\n",
    "\n",
    "\n",
    "def load_data_bicoding(Path):\n",
    "    data = np.loadtxt(Path,dtype=list)\n",
    "    data_result = []\n",
    "    for seq in data:\n",
    "        seq = seq.upper()\n",
    "        seq = str(seq.strip('\\n'))\n",
    "        data_result.append(seq)\n",
    "    return data_result\n",
    "\n",
    "path = '/lizutan/code/MetAc4C/ac4c_data/ac4c_data/ac4c_train_test'\n",
    "\n",
    "pos_train = load_data_bicoding(path+'/ac4c_positive_train.fa')\n",
    "pos_test = load_data_bicoding(path+'/ac4c_positive_test.fa')\n",
    "neg_train = load_data_bicoding(path+'/ac4c_negative_train.fa')\n",
    "neg_test = load_data_bicoding(path+'/ac4c_negative_test.fa')\n",
    "\n",
    "\n",
    "\n",
    "#保存文件路径\n",
    "site_path = '/lizutan/code/MetAc4C/ac4c_data/ac4c_data_bert/'\n",
    "if not os.path.exists(site_path):\n",
    "    os.makedirs(site_path)\n",
    "\n",
    "#=============================================================>>\n",
    "# site_dir = site_path + 'pos_train.csv'\n",
    "# data_pd = make_dataframe(pos_train)\n",
    "\n",
    "# site_dir = site_path + 'pos_test.csv'\n",
    "# data_pd = make_dataframe(pos_test)\n",
    "\n",
    "# site_dir = site_path + 'neg_train.csv'\n",
    "# data_pd = make_dataframe(neg_train)\n",
    "\n",
    "site_dir = site_path + 'neg_test.csv'\n",
    "data_pd = make_dataframe(neg_test)\n",
    "\n",
    "site_dataset = CustomeDataset(data_pd, DefaultArgs)\n",
    "site_dataloader = DataLoader(site_dataset, shuffle=False, drop_last=False, num_workers=8, batch_size=16)\n",
    "model = BertModel.from_pretrained(DefaultArgs.model_path, output_hidden_states=True)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "\n",
    "processed = 0\n",
    "csv_pd = pd.DataFrame()\n",
    "for idx, batch in enumerate(site_dataloader):\n",
    "    index_list,input_array = batch\n",
    "    # index_list  = range(len(input_array))\n",
    "    input_array = input_array.to(\"cuda\")\n",
    "    hidden_states = model(input_array)[-1][-1]\n",
    "    hidden_states = hidden_states.cpu().data.numpy()\n",
    "    hidden_states = hidden_states.reshape(hidden_states.shape[0],-1) #可选，打平 #36096 #[3, 47, 768]\n",
    "    csv_pd = pd.concat([csv_pd,pd.DataFrame(hidden_states)],axis=0)  #将所的表格拼在一起保存\n",
    "\n",
    "    # for array_id, index_name in enumerate(index_list): #每一个条分别保存\n",
    "    #     output_file = \"{}/{}.npz\".format(site_dir, index_name)\n",
    "    #     result = dict(index = index_name,representations = hidden_states[array_id])\n",
    "    #     np.savez(output_file, **result)\n",
    "    \n",
    "    processed += len(index_list)\n",
    "    print(\"\\r{} / {} done!\".format(processed, len(site_dataset)), end='')\n",
    "\n",
    "csv_pd.to_csv(site_dir , index=True,header=True, sep=',')\n",
    "\n",
    "# print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
